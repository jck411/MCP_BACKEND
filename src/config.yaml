# MCP Chatbot Configuration

# WebSocket Chat Configuration
chat:
  # WebSocket server configuration
  websocket:
    host: "localhost"
    port: 8000
    endpoint: "/ws/chat"

    # CORS settings
    allow_origins: ["*"]
    allow_credentials: true

    # WebSocket settings
    max_message_size: 16777216  # 16MB
    ping_interval: 20
    ping_timeout: 10

  # Chat Service Configuration
  service:
    # System prompt configuration
    system_prompt: |
      You are a helpful assistant with a sense of humor.
      You have access to to a list of tools, use them when needed and explain why you used them.

    # Repository configuration (SQLite database)
    repository:
      type: "sql"              # SQLite database storage
      path: "events.db"        # Database file path
      
      # Persistent memory configuration
      persistence:
        enabled: true           # Enable persistent memory storage
        retention_policy: "token_limit"  # "token_limit" | "message_count" | "time_based" | "unlimited"
        
        # Token-based retention (when retention_policy: "token_limit")
        max_tokens_per_conversation: 8000  # Maximum tokens to retain per conversation
        
        # Message-based retention (when retention_policy: "message_count") 
        max_messages_per_conversation: 100  # Maximum messages to retain per conversation
        
        # Time-based retention (when retention_policy: "time_based")
        retention_days: 30      # Days to retain conversation history
        
        # Startup behavior - controls session persistence
        clear_on_startup: true  # Clear all conversations on startup (session-only memory)

    # Context management configuration
    context:
      # Maximum context window size in tokens
      max_tokens: 4000
      
      # Tokens to reserve for LLM response generation
      reserve_tokens: 500
      
      # Conversation length thresholds for response estimation
      conversation_limits:
        short: 100     # Short conversation threshold
        medium: 500    # Medium conversation threshold  
        long: 1500     # Long conversation threshold
      
      # Expected response token counts by conversation length
      response_tokens:
        short: 150     # Expected tokens for short responses
        medium: 300    # Expected tokens for medium responses
        long: 500      # Expected tokens for long responses
        max: 800       # Maximum response tokens for very long conversations
      
      # Conversation optimization settings
      preserve_recent: 5  # Number of recent messages to always preserve

    # Streaming configuration
    streaming:
      enabled: true  # Enable streaming by default (required setting)
      
      # Streaming backoff configuration for file lock handling
      backoff:
        max_attempts: 5        # Maximum number of retry attempts
        initial_delay: 0.05    # Initial backoff delay in seconds
        flush_every_n_deltas: 25  # Flush frequency for streaming deltas

    # Tool execution configuration
    max_tool_hops: 8  # Maximum number of recursive tool calls to prevent infinite loops

    # Tool execution notifications
    tool_notifications:
      enabled: true
      show_args: true
      icon: "ðŸ”§"
      format: "{icon} Executing tool: {tool_name}"
      # Available placeholders: {icon}, {tool_name}, {tool_args}

    # Logging configuration for chat service
    logging:
      tool_execution: true
      tool_results: true
      result_truncate_length: 200
      system_prompt: true  # Log the generated system prompt during initialization
      llm_replies: true  # Log ALL LLM replies including internal ones not sent to user
      llm_reply_truncate_length: 500  # Truncate length for LLM reply logs

# LLM Configuration - Just change the 'active' provider!
llm:
  active: "openai"  # Change this to: openai, groq, openrouter

  # Provider presets
  providers:
    openai:
      base_url: "https://api.openai.com/v1"
      model: "gpt-4o-mini"  # or gpt-4, gpt-4-turbo, etc.
      temperature: 0.7
      max_tokens: 4096
      top_p: 1.0
      
      # Response parsing defaults
      choice_index: 0  # Default choice index for response parsing

    groq:
      base_url: "https://api.groq.com/openai/v1"
      model: "llama-3.3-70b-versatile"  # or llama-3.1-8b-instant, etc.
      temperature: 0.7
      max_tokens: 4096
      top_p: 1.0
      
      # Response parsing defaults
      choice_index: 0  # Default choice index for response parsing

    openrouter:
      base_url: "https://openrouter.ai/api/v1"
      model: "openrouter/horizon-beta"  # or other available models
      temperature: 0.7
      max_tokens: 4096
      top_p: 1.0
      
      # Response parsing defaults
      choice_index: 0  # Default choice index for response parsing



# Logging Configuration
logging:
  level: "INFO"
  format: "%(asctime)s - %(levelname)s - %(message)s"

# MCP Server Configuration
mcp:
  config_file: "servers_config.json"
  
  # Server execution configuration
  server_execution:
    # Default arguments for server commands (explicit empty list)
    default_args: []
    
    # Default environment variables (explicit empty dict)
    default_env: {}
    
    # Server enablement check (require explicit enabled flag)
    require_enabled_flag: true
  
  # Connection and retry configuration
  connection:
    # Maximum number of reconnection attempts per server
    max_reconnect_attempts: 5
    
    # Initial delay between reconnection attempts (seconds)
    # Uses exponential backoff up to max_reconnect_delay
    initial_reconnect_delay: 1.0
    
    # Maximum delay between reconnection attempts (seconds)
    max_reconnect_delay: 30.0
    
    # Connection timeout for initial server connection (seconds)
    connection_timeout: 30.0
    
    # Ping timeout for connection health checks (seconds)
    ping_timeout: 10.0

# Chat Event Store Configuration  
chat_store:
  # Event visibility configuration for LLM context
  visibility:
    # System update visibility (require explicit marking)
    system_updates_visible_to_llm: false
    
    # Default visibility for new event types
    default_visible_to_llm: false
