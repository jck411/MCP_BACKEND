# MCP Chatbot Configuration

# WebSocket Chat Configuration
chat:
  # WebSocket server configuration
  websocket:
    host: "localhost"
    port: 8000
    endpoint: "/ws/chat"

    # CORS settings
    allow_origins: ["*"]
    allow_credentials: true

    # WebSocket settings
    max_message_size: 16777216  # 16MB
    ping_interval: 20
    ping_timeout: 10
    
    # Concurrency and performance configuration (OPTIMIZED FOR HIGH LOAD)
    concurrency:
      # Maximum concurrent WebSocket connections
      max_connections: 1000      # Increased from 100 for high concurrency
      
      # Connection queue settings
      connection_queue_size: 1000  # Must be >= max_connections
      
      # Uvicorn server configuration
      uvicorn:
        backlog: 1200             # Must be >= connection_queue_size
        workers: 1               # Single worker for WebSocket state management
        access_log: false        # Disable for performance
        
        # Keep-alive settings for WebSocket connections
        keepalive_timeout: 120   # 2 minutes for better connection reuse
        max_keepalive_requests: 10000  # High limit for persistent connections
        
        # Server limits (optimized for performance)
        h11_max_incomplete_event_size: 16384  # Max incomplete HTTP event size
        h11_max_request_line_size: 8192       # Max HTTP request line size
        h11_max_header_size: 16384            # Optimized header size

  # Chat Service Configuration
  service:
    # System prompt configuration
    system_prompt: |
      You are a helpful assistant with a sense of humor.
      You have access to to a list of tools, use them when needed and explain why you used them.

    # Repository configuration (SQLite database)
    repository:
      type: "sql"              # SQLite database storage
      path: "events.db"        # Database file path
      
      # Connection pool configuration for high-concurrency SQLite access (OPTIMIZED)
      connection_pool:
        # Pool sizing - total connections = max_readers + max_writers
        max_connections: 20          # Increased pool size for high concurrency
        max_readers: 16              # Many readers for concurrent requests
        max_writers: 4               # Multiple writers for write throughput
        
        # Connection management
        connection_timeout: 30.0     # Balanced timeout for connection acquisition
        health_check_interval: 30.0  # More frequent health checks for reliability
        
        # Performance tuning
        enable_pooling: true         # Enable connection pooling for performance
      
      # Persistent memory configuration
      persistence:
        enabled: true           # Enable persistent memory storage
        retention_policy: "token_limit"  # "token_limit" | "message_count" | "time_based" | "unlimited"
        
        # Token-based retention (when retention_policy: "token_limit")
        max_tokens_per_conversation: 8000  # Maximum tokens to retain per conversation
        
        # Message-based retention (when retention_policy: "message_count") 
        max_messages_per_conversation: 100  # Maximum messages to retain per conversation
        
        # Time-based retention (when retention_policy: "time_based")
        retention_days: 30      # Days to retain conversation history
        
        # Startup behavior - controls session persistence
        clear_on_startup: true  # Clear all conversations on startup (session-only memory)

    # Context management configuration
    context:
      # Maximum context window size in tokens
      max_tokens: 2000  # Reduced from 4000 for faster processing
      
      # Tokens to reserve for LLM response generation
      reserve_tokens: 300  # Reduced from 500
      
      # Conversation length thresholds for response estimation
      conversation_limits:
        short: 100     # Short conversation threshold
        medium: 500    # Medium conversation threshold  
        long: 1500     # Long conversation threshold
      
      # Expected response token counts by conversation length
      response_tokens:
        short: 150     # Expected tokens for short responses
        medium: 300    # Expected tokens for medium responses
        long: 500      # Expected tokens for long responses
        max: 800       # Maximum response tokens for very long conversations
      
      # Conversation optimization settings
      preserve_recent: 5  # Number of recent messages to always preserve

    # Streaming configuration
    streaming:
      enabled: true  # Enable streaming by default (required setting)
      
      # Streaming backoff configuration for file lock handling
      backoff:
        max_attempts: 5        # Maximum number of retry attempts
        initial_delay: 0.05    # Initial backoff delay in seconds
        flush_every_n_deltas: 25  # Flush frequency for streaming deltas
      
      # PERFORMANCE: Delta batch size for database writes
      delta_batch_size: 50     # Increased from 25 for fewer DB operations

    # Tool execution configuration
    max_tool_hops: 8  # Maximum number of recursive tool calls to prevent infinite loops

    # Tool execution notifications
    tool_notifications:
      enabled: true
      show_args: true
      icon: "ðŸ”§"
      format: "{icon} Executing tool: {tool_name}"
      # Available placeholders: {icon}, {tool_name}, {tool_args}

    # Logging configuration for chat service
    logging:
      tool_execution: true
      tool_results: true
      result_truncate_length: 200
      system_prompt: true  # Log the generated system prompt during initialization
      llm_replies: true  # Log ALL LLM replies including internal ones not sent to user
      llm_reply_truncate_length: 500  # Truncate length for LLM reply logs

# LLM Configuration - Just change the 'active' provider!
llm:
  active: "openai"  # Change this to: openai, groq, openrouter

  # Provider presets
  providers:
    openai:
      base_url: "https://api.openai.com/v1"
      model: "gpt-4o-mini"  # or gpt-4, gpt-4-turbo, etc.
      temperature: 0.7
      max_tokens: 4096
      top_p: 1.0
      
      # Response parsing defaults
      choice_index: 0  # Default choice index for response parsing
      
      # HTTP client connection pooling and concurrency limits
      http_client:
        # Connection pool limits for outbound HTTP requests
        max_connections: 50      # Total connection pool size
        max_keepalive: 20        # Connections to keep alive in pool
        keepalive_expiry: 30.0   # Seconds before idle connections expire
        
        # Timeout configuration for HTTP requests
        connect_timeout: 10.0    # Connection establishment timeout
        read_timeout: 60.0       # Response read timeout
        write_timeout: 10.0      # Request write timeout
        pool_timeout: 5.0        # Wait time for connection from pool
        
        # Rate limiting to prevent API overwhelming
        requests_per_minute: 3500    # Max requests per minute
        concurrent_requests: 25      # Max concurrent requests
        
        # Retry configuration for failed requests
        max_retries: 3               # Max retry attempts
        backoff_factor: 0.5          # Exponential backoff multiplier

    groq:
      base_url: "https://api.groq.com/openai/v1"
      model: "llama-3.3-70b-versatile"  # or llama-3.1-8b-instant, etc.
      temperature: 0.7
      max_tokens: 4096
      top_p: 1.0
      
      # Response parsing defaults
      choice_index: 0  # Default choice index for response parsing
      
      # HTTP client connection pooling and concurrency limits
      http_client:
        # Connection pool limits for outbound HTTP requests
        max_connections: 30      # Total connection pool size (lower for Groq)
        max_keepalive: 10        # Connections to keep alive in pool
        keepalive_expiry: 20.0   # Seconds before idle connections expire
        
        # Timeout configuration for HTTP requests
        connect_timeout: 10.0    # Connection establishment timeout
        read_timeout: 90.0       # Response read timeout (higher for Groq)
        write_timeout: 10.0      # Request write timeout
        pool_timeout: 5.0        # Wait time for connection from pool
        
        # Rate limiting to prevent API overwhelming (Groq specific limits)
        requests_per_minute: 600     # Max requests per minute (Groq limit)
        concurrent_requests: 15      # Max concurrent requests
        
        # Retry configuration for failed requests
        max_retries: 3               # Max retry attempts
        backoff_factor: 1.0          # Exponential backoff multiplier

    openrouter:
      base_url: "https://openrouter.ai/api/v1"
      model: "google/gemini-2.5-flash"  # or other available models
      temperature: 0.7
      max_tokens: 4096
      top_p: 1.0
      
      # Response parsing defaults
      choice_index: 0  # Default choice index for response parsing
      
      # HTTP client connection pooling and concurrency limits
      http_client:
        # Connection pool limits for outbound HTTP requests
        max_connections: 20      # Reduced from 40 for lower overhead
        max_keepalive: 8         # Reduced from 15
        keepalive_expiry: 15.0   # Reduced from 25.0
        
        # Timeout configuration for HTTP requests
        connect_timeout: 8.0     # Reduced from 15.0
        read_timeout: 60.0       # Reduced from 120.0
        write_timeout: 8.0       # Reduced from 10.0
        pool_timeout: 3.0        # Reduced from 8.0
        
        # Rate limiting to prevent API overwhelming
        requests_per_minute: 2000    # Max requests per minute
        concurrent_requests: 20      # Max concurrent requests
        
        # Retry configuration for failed requests
        max_retries: 3               # Max retry attempts
        backoff_factor: 0.8          # Exponential backoff multiplier



# Logging Configuration
logging:
  level: "INFO"
  format: "json"  # "json" for production, "console" for development
  service_name: "mcp-platform"
  service_version: "1.0.0"

# MCP Server Configuration
mcp:
  config_file: "servers_config.json"
  
  # Server execution configuration
  server_execution:
    # Default arguments for server commands (explicit empty list)
    default_args: []
    
    # Default environment variables (explicit empty dict)
    default_env: {}
    
    # Server enablement check (require explicit enabled flag)
    require_enabled_flag: true
  
  # Connection and retry configuration
  connection:
    # Maximum number of reconnection attempts per server
    max_reconnect_attempts: 5
    
    # Initial delay between reconnection attempts (seconds)
    # Uses exponential backoff up to max_reconnect_delay
    initial_reconnect_delay: 1.0
    
    # Maximum delay between reconnection attempts (seconds)
    max_reconnect_delay: 30.0
    
    # Connection timeout for initial server connection (seconds)
    connection_timeout: 30.0
    
    # Ping timeout for connection health checks (seconds)
    ping_timeout: 10.0

# Chat Event Store Configuration  
chat_store:
  # Event visibility configuration for LLM context
  visibility:
    # System update visibility (require explicit marking)
    system_updates_visible_to_llm: false
    
    # Default visibility for new event types
    default_visible_to_llm: false
